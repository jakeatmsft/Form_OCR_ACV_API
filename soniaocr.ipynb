{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "# If you are using a Jupyter notebook, uncomment the following line.\n",
    "#%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib .patches import Polygon\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# Replace <Subscription Key> with your valid subscription key.\n",
    "subscription_key = \"2558f198d2384bd0800ea00bd5d26dc6\"\n",
    "assert subscription_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You must use the same region in your REST call as you used to get your\n",
    "# subscription keys. For example, if you got your subscription keys from\n",
    "# westus, replace \"westcentralus\" in the URI below with \"westus\".\n",
    "#\n",
    "# Free trial subscription keys are generated in the \"westus\" region.\n",
    "# If you use a free trial subscription key, you shouldn't need to change\n",
    "# this region.\n",
    "vision_base_url = \"https://westus.api.cognitive.microsoft.com/vision/v2.0/\"\n",
    "#analyze_url = vision_base_url + \"analyze\"\n",
    "\n",
    "text_recognition_url = vision_base_url + \"recognizeText\"\n",
    "\n",
    "# Set image_url to the URL of an image that you want to analyze.\n",
    "#image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/\" + \\\n",
    "    #\"Cursive_Writing_on_Notebook_paper.jpg/800px-Cursive_Writing_on_Notebook_paper.jpg\"\n",
    "    \n",
    "image_path = \"C:/Users/sonan/Pictures/soniaocrtest1.jpg\"\n",
    "\n",
    "# Read the image into a byte array\n",
    "image_data = open(image_path, \"rb\").read()\n",
    "#image_data = open(image_path, encoding=\"utf8\", errors='ignore').read()\n",
    "#image_data = image_data.rstrip(\"\\n\").decode(\"utf-16\")\n",
    "#image_data = image_data.split(\"\\r\\n\")\n",
    "\n",
    "\n",
    "\n",
    "#headers = {'Ocp-Apim-Subscription-Key': subscription_key}\n",
    "headers = {'Ocp-Apim-Subscription-Key': subscription_key,'Content-Type': 'application/octet-stream'}\n",
    "# Note: The request parameter changed for APIv2.\n",
    "# For APIv1, it is 'handwriting': 'true'.\n",
    "params  = {'mode': 'Handwritten'}\n",
    "#data    = {'url': image_url}\n",
    "response = requests.post(\n",
    "    text_recognition_url, headers=headers, params=params, data=image_data)\n",
    "response.raise_for_status()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting handwritten text requires two API calls: One call to submit the\n",
    "# image for processing, the other to retrieve the text found in the image.\n",
    "\n",
    "# Holds the URI used to retrieve the recognized text.\n",
    "operation_url = response.headers[\"Operation-Location\"]\n",
    "\n",
    "# The recognized text isn't immediately available, so poll to wait for completion.\n",
    "analysis = {}\n",
    "poll = True\n",
    "while (poll):\n",
    "    response_final = requests.get(\n",
    "        response.headers[\"Operation-Location\"], headers=headers)\n",
    "    analysis = response_final.json()\n",
    "    time.sleep(1)\n",
    "    if (\"recognitionResult\" in analysis):\n",
    "        poll= False  \n",
    "    if (\"status\" in analysis and analysis['status'] == 'Failed'):\n",
    "        poll= False\n",
    "\n",
    "polygons=[]\n",
    "if (\"recognitionResult\" in analysis):\n",
    "    # Extract the recognized text, with bounding boxes.\n",
    "    polygons = [(line[\"boundingBox\"], line[\"text\"])\n",
    "        for line in analysis[\"recognitionResult\"][\"lines\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_image = Image.open(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   # Display the image and overlay it with the extracted text.\n",
    "plt.figure(figsize=(15, 15))\n",
    "image = Image.new(\"RGBA\", old_image.size, \"white\")\n",
    "display(old_image)\n",
    "ax = plt.imshow(image)\n",
    "for polygon in polygons:\n",
    "    vertices = [(polygon[0][i], polygon[0][i+1])\n",
    "        for i in range(0, len(polygon[0]), 2)]\n",
    "    text     = polygon[1]\n",
    "    patch    = Polygon(vertices, closed=True, fill=False, linewidth=2, color='y')\n",
    "    ax.axes.add_patch(patch)\n",
    "    plt.text(vertices[0][0], vertices[0][1], text, fontsize=20, va=\"top\")\n",
    "_ = plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
